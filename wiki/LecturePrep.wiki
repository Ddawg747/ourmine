#summary Pre-processing methods

The truth of the matter is that most of the time spent in data mining is really spent on data pre-processing.

<img width=600 src="http://ourmine.googlecode.com/svn/trunk/share/img/effort.gif">

So lets talk about pre-processing.

<wiki:toc max_depth="4" />

= What is an Instance? =

In sample, classical learning, the input is usually a "flat file"; e.g.:
{{{
(defun contact-lens ()
  (data
   :name 'contact-lens
   :columns '(age prescription astigmatism tear-production lens)
   :egs
   '(
     (young myope yes normal hard)
     (young hypermetrope yes normal hard)
     (presbyopic myope yes normal hard)
     (pre-presbyopic myope yes normal hard)
     
     (young hypermetrope no reduced none)
     (young hypermetrope yes reduced none)
     (pre-presbyopic hypermetrope yes reduced none)
     (pre-presbyopic hypermetrope yes normal none)
     (young myope no reduced none)
     (young myope yes reduced none)
     (presbyopic myope no reduced none)
     (presbyopic myope no normal none)
     (presbyopic hypermetrope yes reduced none)
     (presbyopic hypermetrope yes normal none)
     (presbyopic myope yes reduced none)
     (pre-presbyopic hypermetrope no reduced none)
     (pre-presbyopic myope no reduced none)
     (pre-presbyopic myope yes reduced none)
     (presbyopic hypermetrope no reduced none)
     
     (pre-presbyopic myope no normal soft)
     (pre-presbyopic hypermetrope no normal soft)
     (young myope no normal soft)
     (young hypermetrope no normal soft)
     (presbyopic hypermetrope no normal soft)
     )))
}}}

This data has the following properties:
    * Denomormalized (i.e. joins across multiple separate data sources)
    * Not the best representation
    * e.g. trees are hard; 
          * Not everyone has parents. What to write into the file?
          * What about all the implicit relations; e,g, sister
          * Pre-compute and cache in the ARFF?
          * Bad! Massive blow-out of data size.

http://ourmine.googlecode.com/svn/trunk/share/img/family_tree.jpg

= Numeric Transforms =

Pragmatically, the following transforms for numbers are very useful:

 * Coarse-grain: if many numbers, bunch them up into a small set; e.g. below mean, above mean
    * More generally, discretize them (see [#Discretization[). 

 * Log: if the numerics form an exponential distribution, convert all N to log(N)
    * Used in the COCOMO example
    * pragmatics: missing values can't be "zero" (infinitely negative log values)
    * Apply some min value (e.g. 0.0001) and use:
    * new = (if old < min then log(min) else log(old))
 * Remove outliers: suspiciously large/small values
 * Replace unknowns with most expected  value
    * Numerics: use mean or median
	* Discretes: use most common symbol
    * ?cluster first, then fill in from local neighborhood
    * Warning: maybe missing really means missing!
    * Also, some learners can handle missing valus.
 * Bore: Best or Rest
    * Sort: rename the top X% (best) values "good" and the rest "bad"; eg. X=25%
    * Scales to multi-variables (can be used to replace multiple numeric target classes with one binary classification)
 *  Time series 
    * Add attributes to record the moving average over the last N minutes, 5N minutes, 25N minutes, etc

= Sampling =
Build data sets by sub-sampling real data

    * Column sub-sampling: manual FeatureSelection
          o Maybe there is domain knowledge that some columns are
          o More costly to use
          o Less trustworthy
          o E.g. in the MDP data: different measures from modules
    * Row sampling: manual [[Stratification]]
          o Ignore all but the relevant data (how to judge? domain knowledge? nearest neighbor?).

http://ourmine.googlecode.com/svn/trunk/share/img/samples.png

When the target class is rare,

    * Sub-sample: create training sets that contain all the target instances and an equal number of randomly selected non-target instances (stand back! give the little guy some air!).
    * Super-sample: take the minority class and repeat it (build yourself up in the crowd). 


Some experimental results:

    * In one study, under-sampling beat over-sampling [Drummond03].
    * In another, once again, over-sampling was useless and under-sampling did the same as no-sampling, but with much much less data [Menzies08a]. The following results show balance results for the above data sets. NB= naive bayes. J48= a decision tree learner.

<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/overunder.png">



= Standard Text Transforms =

Tokenize (kill white space).

Send to lower case.

Remove stop words (boring words like "a above above.." (see examples at http://www.dcs.gla.ac.uk/idom/irresources/linguisticutils/stop_words

    * Warning: don't remove words that are important to your domain.

Stemming

    * These words are all "connect": connect, connected, connecting, connection,connections
    * PorterStemming: the standard stemming algorithm, available in multiple languages: http://www.tartarus.org/~martin/PorterStemmer/
          * Definition: http://www.tartarus.org/~martin/PorterStemmer/def.txt

Reject all but the top k most interesting words

    * Interesting if frequent OR usually appears in just a few paragraphs
    * TF{{{*}}}IDF (term frequency, inverse document frequency)
    * Interesting =
      F(i,j) {{{*}}} log((Number of documents)/(number of documents including word i))
    * F(i,j): frequency of word i in document j
    * Often, on a very small percentage of the words are high scorers, so a common transform is to just use the high fliers. e.g. here's data from five bug tracking systems a,b,c,d,e:

<img src="http://ourmine.googlecode.com/svn/trunk/share/img/tfidf.png" width=400>

Build a symbol table of all the remaining words

    * Convert strings to pointers into the symbol table
    * So "the cat sat on the cat" becomes 6 pointers to 4 words

= Other =

    * Watch for one-letter typos
          o Check all symbols that occur only once in the data.
    * Add synthetic attributes
          o Capture domain knowledge
          o e.g. risk is age/weight*temperature
    * Sample randomly
          o Useful when the whole data can't fit into ram.
          o Useful when building training/test sets
    * Sample instances according to the mis-classification rate of its class
          o So focus more on the things that are harder to classify
          o Also called Boosting: discussed (much) later
