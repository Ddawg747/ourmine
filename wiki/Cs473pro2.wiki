#summary intermediary data mining techniques

Due Monday week 7 

For this, and the next project, everyone has to code up

 * two pre-processors
 * two discretizers
 * two clusterers
 * two classifiers
 * two feature subset selectors.

Note that:

 * There are some magic numbers associated with each of the above  functions- assume three settings for each different data miners. 
 * Which you'll repeat 10 times;
 * Over 10 data sets
 * This means that, before the end of term, you'll have to run this code 10*10*3^5^*2^5^=777 combinations.
 * Tee hee!

Future projects will comparatively assess these 20,000+ possible data miners. This project will just get you started.

You will assume that there exists two files "train.lisp" and "test.lisp" containing data in the same format (same number of columns, and if a column is numeric/discrete in one, it is numeric/discrete in the other).

To run the code, your main function must be

{{{
(defun learn (&key (k            8)
                   (prep         #'normalizeNumerics)
                   (discretizer  #'10bins)
                   (cluster      #'(lambda (data) (kmeans k data)) 
                   (fss          #'infoGain)
                   (classify     #'naiveBayes)
                   (train        "train.lisp")
                   (test         "test.lisp"))
     ; first prep train and test
     ; then run the discretizer
     ; then cluster the training set into k clusters
     ; then train a classifier for each cluster
     ; then for all example in the test set
     ;     ... find the cluster with the nearest centroid...
     ;     ... classify that example  using that cluster's classifier
}}}
When you run the testing, you must keep four separate counts, for each class X in the test
set.


* A: the number of test things  that are NOT class=X which were NOT classified as class=X
* B: the number of test things  that ARE class=X which were NOT classified as class=X
* C: the number of test things  that ARE class=X where were NOT classified as class=X


From 
                  do it ain't    yes, it is   
                  |-----------|-------------|
detector said no  |     A     |       B     |
                  |-----------|-------------|
detector said yes |     C     |       D     |
                  |-----------|-------------|

    * prec= precision   = D / (C+D)
    * acc= accuracy   = (A+D) / (A+B+C+D)
    * pd= probability of detection   = pd = D / (B+D)
    * pf= probability of false alarm   C / (A+C)
    * f= f-measure = 2(prec)(recall) / (prec + recall)
    * g= g-measure = 2(pf)(pd)  / (pf+pd)

Your output must be comma separated lines, one for each class in test set.

{{{
prep,discretizer, cluster,fss,classify, class,a,b,c,d,acc,prec,pd,pf,f,g <NEWLINE>
}}}

= Introduction =

Add your content here.


= Details =

Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages