#summary Evaluation Bias


All learning is biased. After {search, language, over-fitting, sampling} bias comes a new one:

    * evaluation bias
    * the way we evaluate a learned theory determines what theories we accept or reject.

This lecture - some of the issues associated with evaluating a theory.

Anyway, how to evaluate learning output:

    * Answer #1: assume something about the distributions, then use "t-tests" and "win-loss" tables.
    * Answer #2: use non-parametric methods (that make no such assumptions).
          * e.g. quartile charts
          * e.g. ranked statistical methods (Mann-Whitney, Wilcoxon)

But before we can evaluate N learners, we need to gather performance statistics from one learner.

= Performance Statistics =

== Discrete classes ==

Simplest case: two discrete classes.
{{{
                  truth = no   truth = yes  
                 |-----------|-------------|
detector = silent|     A     |       B     |
detector = loud  |     C     |       D     |
                 |-----------|-------------|
}}}
    * precision = D / (C+D)
    * accuracy = (A+D) / (A+B+C+D)
    * probability of detection = D / (B+D) (a.k.a. recall)
    * probability of false alarm = C / (A+C)

<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/rocsheet.png">

Harder case:

    * more than two classes
    * repeat the above, one for each pair of (class X) / (not class X).

<img width=300 src="http://ourmine.googlecode.com/svn/trunk/share/img/rocsheets.png">

= Continuous classes =

Evaluation measures include RE, MRE, MMRE, PRED(N), correlation, etc.

    * RE = relative error = (predicted - actual) / actual
    * MRE = magnitude of relative error = absolute value of RE
    * PRED(N) = % of cases where MRE up to N%
          * e.g. MRE from ten instances = 11,15,18,20,23,25,29,31,100,100
                * then the PRED(30) = 80%
    * MMRE = mean magnitude of relative error = average MRE over N test instances = sum(MRE)/N
          * e.g. MRE from ten instances = 11,15,18,20,23,25,29,31,100,100
                * then the MMRE = 37.2%
    * correlation = corr
          * if corr =0, no connection dependent on independents
          * if corr = 1, perfect dependence of dependent on independents
          * if corr = -1, perfect negative dependence of dependent on independents
          * if corr in -0.5 .. 0.5 then correlation is not strong
          * calculating correlation for N items stored in arrays x and y:

<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/correlation.png">
{{{
sum_sq_x = 0
sum_sq_y = 0
sum_coproduct = 0
mean_x = x[1]
mean_y = y[1]
for i in 2 to N:
      sweep = (i - 1.0) / i
      delta_x = x[i] - mean_x
      delta_y = y[i] - mean_y
      sum_sq_x += delta_x * delta_x * sweep
      sum_sq_y += delta_y * delta_y * sweep
      sum_coproduct += delta_x * delta_y * sweep
      mean_x += delta_x / i
      mean_y += delta_y / i 
pop_sd_x = sqrt( sum_sq_x / N )
pop_sd_y = sqrt( sum_sq_y / N )
cov_x_y = sum_coproduct / N
correlation = cov_x_y / (pop_sd_x * pop_sd_y)
}}}

= Evaluation bias (again) =

How to combine (say) PD and PF (different applications have different cost/dependability requirements).
<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/roc.png">

    * A [http://code.google.com/p/ourmine/wiki/Mm#Menzies07a recent study] of defect detectors used:
          * Note: generalizes to N-dimensional evaluation (divide by sqrt(N))
<img width=300 src="http://ourmine.googlecode.com/svn/trunk/share/img/balance.png">
          * More generally, it may be useful to weight {PD,PF} according to their relative importance.
    * Also, if we have N theories (say _defects1 > T1 and defects2 > T2)), each with their own ROC-curve shape, we can use them in combination to design learners with some desired ROC curves.  
<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/rocs.png">

= Cross-Validation =

Self-tests (testing the learned theory on its own training data) is deprecated by the data mining community since such self-tests can grossly over-estimate performance.

    * If the goal is to understand how well a defect predictor will work on future projects, it is best to evaluate the predictor via hold out instances not used in the generation of that predictor.

One such hold-out study is an M*N-way cross-val.

    * Usually, M=N=10
    * This requires M{{{*}}}N+1 runs to the learner.
          * run1 : generate a theory from all the data
          * remaining runs: find an estimate for the error of that theory
          * repeat M times
                * randomize order of data (avoid order effects)
                * 10 times, generate 1/N test and (N-1)/N training sets
                * train on training, apply learned theory to testing
    * note: error on test sets will over-estimate error of the training set
          * R= number of training instances
          * h= a learned specific parameter (often, not knowable)
          * At probability 1-n:
<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/vc.png">
                * That is, larger training sets reduce error

Incremental M*N-way (a.k.a. sequence studies)

    * Learn on more and more of the data
    * Report "plateau" (where learning improvement stops)
    * Repeat M times
          * Divide data into buckets on N items
          * For I = 2 to N
                * Training on buckets 1..I-1
                * Test on bucket I

In many of the standard UCI data sets, learning plateaus long before data is exhausted. Often, 200 instances are enough:

<img width=600 src="http://ourmine.googlecode.com/svn/trunk/share/img/plateau.png">

<img width=600 src="http://ourmine.googlecode.com/svn/trunk/share/img/platueas.png">

(BTW: the conclusion that learners plateau after a few hundred instances has not been reported widely in the literature. Use this conclusion with care, with a pinch of salt, with healthy scepticism.)

= Comparatively Evaluate N learners =
== Quartile Charts ==

Returning now to the point of this lecture (how to evaluate results from multiple learners):

    * Previously, this class has been shown how to generate deltas in the behavior of different learners/pre-processors in cross-validation studies.
    * To generate these charts, the performance deltas for some method were sorted to find the lowest and highest quartile as well as the median value.

For example, suppose we generates this log file:

{{{
    #data, learner, goal, train, test, repeats, bin, accuracy,    pd,    pf, precision
q2s/sonar,   bayes, Mine,   139,   69,       1,   1,    63.77, 61.29, 34.21,     59.38
q2s/sonar,    oneR, Mine,   139,   69,       1,   1,    68.12, 61.90, 22.22,     81.25
q2s/sonar,   bayes, Mine,   139,   69,       1,   2,    60.87, 73.08, 46.51,     48.72
q2s/sonar,    oneR, Mine,   139,   69,       1,   2,    68.12, 68.89, 33.33,     79.49
q2s/sonar,   bayes, Mine,   139,   69,       1,   3,    73.91, 86.67, 35.90,     65.00
q2s/sonar,    oneR, Mine,   139,   69,       1,   3,    69.57, 72.09, 34.62,     77.50
etc 
}}}

We can convert this into quartile charts. To generate these charts, the performance differences for some method on the same data subsets are sorted to find the lowest and highest quartile as well as the median value; e.g


This produces a little ascii bar chart where:

    * the median is shown as a vertical bar "|".
    * the first and fourth quartiles are shown as starts "*".
    * the second and third quartiles are the gaps between the stars and the median.


{{{
        #rx, mean,   sd, n,  min,   q1,median,   q3,  max,....................0....................
   oneR/cat,  1.2,  8.0,54,-21.7, -4.3,   4.3,  6.2,  8.7,................****.|...................
 oneR/nbins,  0.0,  0.0, 3,  0.0,  0.0,   0.0,  0.0,  0.0,....................|....................
  bayes/cat,  0.0,  0.0,18,  0.0,  0.0,   0.0,  0.0,  0.0,....................|....................
bayes/nbins, -1.2,  8.0,54, -8.7, -6.2,  -4.9,  4.3, 21.7,..................*|.****................
}}}

This is a little hard to read so we can use a little latex magic to generate a pdf. The preamble
contains:
{{{
\usepackage{colortbl}

\newcommand{\boxplot}[5]{
\begin{picture}(100, 7)
\put(#1, 2){\line(0, 1){4}}
\put(#1, 4){\line(1, 0){#2}}
\put(#3, 4){\circle*{3}}
\put(#3, 4){\line(1, 0){#4}}
\put(#5, 2){\line(0, 1){4}}
\put(50, 3){\line(0, 1){4}}
\end{picture}
}
}}}
Then we can generate this:

<img width=600 src="http://ourmine.googlecode.com/svn/trunk/share/img/latexQuartiles.png">
{{{
\begin{figure}[!t]
\renewcommand{\baselinestretch}{0.5}
\noindent
{\scriptsize
\begin{tabular}{c r  @{} c }
\multicolumn{3}{c}{Flight} \\\hline

Rank & Change  & 50\% \\
\hline
\rowcolor[rgb]{0.8,0.8,0.8}1 & flight2reuse, & \boxplot{17.700000}{3.500000}{21.200000}{12.100000}{33.300000} \\
\rowcolor[rgb]{0.8,0.8,0.8}2 & improveteam, & \boxplot{20.800000}{4.500000}{25.300000}{9.100000}{34.400000} \\
\rowcolor[rgb]{0.8,0.8,0.8}2 & none, & \boxplot{20.100000}{6.200000}{26.300000}{11.800000}{38.100000} \\
3 & reducefunc, & \boxplot{18.700000}{8.500000}{27.200000}{11.000000}{38.200000} \\
3 & improveprecflex, & \boxplot{22.400000}{7.700000}{30.100000}{11.800000}{41.900000} \\
3 & flight4reuse, & \boxplot{23.000000}{7.200000}{30.200000}{12.400000}{42.600000} \\
4 & relaxschedule, & \boxplot{21.000000}{7.500000}{28.500000}{18.400000}{46.900000} \\
4 & archriskresl, & \boxplot{21.900000}{7.900000}{29.800000}{10.200000}{40.000000} \\
5 & improvepmat, & \boxplot{21.200000}{10.000000}{31.200000}{15.100000}{46.300000} \\
6 & flight3reuse, & \boxplot{23.800000}{10.600000}{34.400000}{10.400000}{44.800000} \\
7 & reducequality, & \boxplot{30.600000}{5.500000}{36.100000}{11.400000}{47.500000} \\
8 & improvepcap, & \boxplot{29.600000}{9.800000}{39.400000}{8.100000}{47.500000} \\
9 & improvetooltechplat, & \boxplot{40.800000}{13.000000}{53.800000}{18.400000}{72.200000} \\
10& flight1reuse, & \boxplot{31.400000}{35.500000}{66.900000}{14.700000}{81.600000} \\

\end{tabular}~\begin{tabular}{|c r    @{} c  }
\multicolumn{3}{|c}{Ground} \\\hline

Rank & Change &  50\% \\
\hline
\rowcolor[rgb]{0.8,0.8,0.8}1 & ground1reuse, & \boxplot{15.400000}{5.100000}{20.500000}{28.500000}{49.000000} \\
\rowcolor[rgb]{0.8,0.8,0.8}2 & ground4reuse, & \boxplot{22.700000}{6.400000}{29.100000}{6.500000}{35.600000} \\
\rowcolor[rgb]{0.8,0.8,0.8}2 & ground3reuse, & \boxplot{23.300000}{6.000000}{29.300000}{7.300000}{36.600000} \\
\rowcolor[rgb]{0.8,0.8,0.8}2 & improvepmat, & \boxplot{22.700000}{9.400000}{32.100000}{5.100000}{37.200000} \\
\rowcolor[rgb]{0.8,0.8,0.8}2 & improveteam, & \boxplot{21.900000}{10.400000}{32.300000}{9.900000}{42.200000} \\
3 & archriskresl, & \boxplot{26.100000}{7.800000}{33.900000}{11.100000}{45.000000} \\
4 & none, & \boxplot{29.100000}{5.000000}{34.100000}{7.600000}{41.700000} \\
4 & relaxschedule, & \boxplot{30.800000}{2.700000}{33.500000}{7.800000}{41.300000} \\
4 & improveprecflex, & \boxplot{24.400000}{13.700000}{38.100000}{7.500000}{45.600000} \\
4 & improvepcap, & \boxplot{25.500000}{10.800000}{36.300000}{10.600000}{46.900000} \\
5 & reducefunc, & \boxplot{27.800000}{6.900000}{34.700000}{8.000000}{42.700000} \\
6 & reducequality, & \boxplot{27.200000}{10.700000}{37.900000}{6.900000}{44.800000} \\
7 & ground2reuse, & \boxplot{25.700000}{23.300000}{49.000000}{30.400000}{79.400000} \\
8 & improvetooltechplat, & \boxplot{38.800000}{20.100000}{58.900000}{6.000000}{64.900000} \\
\end{tabular}
}

\caption{EFFORT: staff months (normalized 0..100\%):  top-ranked changes are shaded.}
\label{fig:effort}
\end{figure}
}}}


In a recent study of defect detectors this code was used to plot 100,000s of results into a very small space:

<img width=500 src="http://ourmine.googlecode.com/svn/trunk/share/img/quartile.png">



From this diagram, we seek stand-out results.
    * A stand-out effect is a large and positive median with a highest quartile bunched up towards the maximum figure.
    * A negative stand-out is similar, but highly skewed towards the negative.

In the above diagram, the logNums.nb are a stand-out effect since:
    * it has much higher medians than all the rest
    * the upper quartile is highly skewed towards the maximum

The oneR results are a negative stand-out since:
    * it is highly skewed towards the negative.
    * In fact, it never does better than anything else, ever (since none of its deltas are positive).

My preference for quartile charts stems from several factors:
    * They can summarize very large experiments very succinctly;
    * Unlike t-tests (discussed below), they make no assumptions the underlying distributions;
    * They only show that method1 is better than method2 if method1 exhibits stand-out effects.

T-tests, on the other hand, condone conclusions like "at the 95% confidence level a difference of 2% in this methods was shown to be statistically significant".
    * Sometimes, "statistically significant" is actually quite insignificant and unimpressive.
    * And t-tests have their own subtle problems.

Students of data mining should be aware that this view of mine is an (extremely) minority view. So, unless you want to be an army of one, I'd best teach you about the standard way the performance of different learners are evaluated; i.e. t-tests and win-loss tables.

= T-Tests & Win-loss tables =

When Hall & Holmes report the results of their feature subset selectors, they present the following table of mean performances:

<img width=500 src="http://ourmine.googlecode.com/svn/trunk/share/img/resultshh.png">

Note the white and black circles comparing the column one figures to the rest. This show results that are significantly different- a concept we will explore in just a second.

Hall & Holmes also report their studies in terms of win-loss tables that compare pairs of performance between learners.

    * If the results are not significantly different, there is a tie (one point to each item in the pair)
    * If the results are significantly different, then someone losses (one point) and someone wins (one point).
{{{
      function winLossTie(i,j,n,sum,sumSq, comp) {
          comp= compare(mean(n,sum), sd(n,sum,sumSq), n, n-1)
          if (comp == 0) { Tie[i]++;  Tie[j]++;  }
          if (comp >  0) { Win[i]++;  Loss[j]++; }
          if (comp <  0) { Loss[i]++; Win[j]++;  }
      }
}}}
    * These results are sorted in the order win - loss
<img width=200 src="http://ourmine.googlecode.com/svn/trunk/share/img/winloss.png">

Such win-loss tables :

    * Are succinct (very)
    * But:
          * They ignore the size of the wins and losses (something featured in the quartile charts)
                * on the other hand, some argue that this is a feature, not a bug
          * They makes certain assumptions about the underlying distributions
                * and, just recently, I have been badly burned by blindly applying t-tests without looking at the underlying distribution.
                * So I do both quartile charts and win-loss tables.

Anyway, what does statistically different mean?

    * When testing for different distributions, the higher the confidence, the more we demand that the distributions don't overlap. Standard practice is to try either a 95% or 99% confidence levels.
    * This test requires making an assumption of the underlying distribution. If we assume a normal or Gaussian curve, then we can compute the mean and spread (a.k.a. standard deviation) of that distribution in a single pass of the data as follows:
          * let n be the number of values;
          * let sum be the sum of the values; i.e. sum = sum + value;
          * let sumSq be the sum of the square of the values; i.e. sumSq = sumSq + value*value
          * let the mean and standard deviation be calculated as follows:
{{{
function mean(sum,n)  {
     return sum/n 
}
function standardDeviation(sumSq,sum,n)  {
     return sqrt((sumSq-((sum*sum)/n))/(n-1)) 
}
}}}
(BTW, A "normal" or Gaussian distribution has a middle value (the mean) and spreads out evenly on both sides by an amount controlled by the standard deviation: large deviations lead to broad curves (e.g. the blue curve) and small deviations lead to sharply falling curves (e.g. the red curve).
)

<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/normals.png">

To test if two distributions are different, we assume that they are normal curves and seek all pairs of performance deltas and study the mean and standard deviation as follows:
{{{
function tval(mean,sd,n)  { 
    return mean/(sd/sqrt(n)) 
}
}}}
If this fraction is small enough, then:

    * the mean of the differences is small, so the two distributions are the same;
    * OR the standard deviation is large so you can't really distinguish the two distributions;
    * OR the sample sample size n is very large so even if we have small mean differences and large standard deviations, there are enough examples around to show that even small observed differences denote actual differences in the distribution.

But what does small enough mean? The answer depends on how large is your sample and how confident you want to be. High confidence and small sample sizes need LARGE tval differences to demonstrate their differences:
{{{
BEGIN { 
    ## T[k,c]
    #  k = degrees of freedom = n = 1
    #  if c = 0.05, then confidence = 95%
    #  if c = 0.01, then confidence = 99%
    T2[1,0.05] = 12.706;    T2[1,0.01] = 63.6570;
    T2[2,0.05] = 4.3030;    T2[2,0.01] = 9.9250;
    T2[3,0.05] = 3.1820;    T2[3,0.01] = 5.8410;
    T2[4,0.05] = 2.7760;    T2[4,0.01] = 4.6040;
    T2[5,0.05] = 2.5710;    T2[5,0.01] = 4.0320;
    T2[6,0.05] = 2.4470;    T2[6,0.01] = 3.7070;
    T2[7,0.05] = 2.3650;    T2[7,0.01] = 3.4990;
    T2[8,0.05] = 2.3060;    T2[8,0.01] = 3.3550;
    T2[9,0.05] = 2.2620;    T2[9,0.01] = 3.2500;
    T2[10,0.05] = 2.2280;   T2[10,0.01] = 3.1690;
    T2[11,0.05] = 2.2010;   T2[11,0.01] = 3.1060;
    T2[12,0.05] = 2.1790;   T2[12,0.01] = 3.0550;
    T2[13,0.05] = 2.1600;   T2[13,0.01] = 3.0120;
    T2[14,0.05] = 2.1450;   T2[14,0.01] = 2.9770;
    T2[15,0.05] = 2.1310;   T2[15,0.01] = 2.9470;
    T2[16,0.05] = 2.1200;   T2[16,0.01] = 2.9210;
    T2[17,0.05] = 2.1100;   T2[17,0.01] = 2.8980;
    T2[18,0.05] = 2.1010;   T2[18,0.01] = 2.8780;
    T2[19,0.05] = 2.0930;   T2[19,0.01] = 2.8610;
    T2[20,0.05] = 2.0860;   T2[20,0.01] = 2.8450;
    T2[21,0.05] = 2.0800;   T2[21,0.01] = 2.8310;
    T2[22,0.05] = 2.0740;   T2[22,0.01] = 2.8190;
    T2[23,0.05] = 2.0690;   T2[23,0.01] = 2.8070;
    T2[24,0.05] = 2.0640;   T2[24,0.01] = 2.7970;
    T2[25,0.05] = 2.0600;   T2[25,0.01] = 2.7870;
    T2[26,0.05] = 2.0560;   T2[26,0.01] = 2.7790;
    T2[27,0.05] = 2.0520;   T2[27,0.01] = 2.7710;
    T2[28,0.05] = 2.0480;   T2[28,0.01] = 2.7630;
    T2[29,0.05] = 2.0450;   T2[29,0.01] = 2.7560;
    T2[30,0.05] = 2.0420;   T2[30,0.01] = 2.7500;
    T2[35,0.05] = 2.03;     T2[35,0.01] = 2.72;
    T2[40,0.05] = 2.02;     T2[40,0.01] = 2.7;
    T2[45,0.05] = 2.01;     T2[45,0.01] = 2.69;
    T2[50,0.05] = 2.01;     T2[50,0.01] = 2.68;
    T2[55,0.05] = 2;        T2[55,0.01] = 2.67;
    T2[60,0.05] = 2;        T2[60,0.01] = 2.66;
    T2[65,0.05] = 2;        T2[65,0.01] = 2.65;
    T2[70,0.05] = 1.99;     T2[70,0.01] = 2.65;
    T2[75,0.05] = 1.99;     T2[75,0.01] = 2.64;
    T2[80,0.05] = 1.99;     T2[80,0.01] = 2.64;
    T2[85,0.05] = 1.99;     T2[85,0.01] = 2.63;
    T2[90,0.05] = 1.99;     T2[90,0.01] = 2.63;
    T2[95,0.05] = 1.99;     T2[95,0.01] = 2.63;
    T2[100,0.05] = 1.98;    T2[100,0.01] = 2.63;
    T2[200,0.05] = 1.97;    T2[200,0.01] = 2.6;
    T2[500,0.05] = 1.96;    T2[500,0.01] = 2.59;
    T2[1000,0.05] = 1.96;   T2[1000,0.01] = 2.58;
    T2["inf",0.05] = 1.96;  T2["inf",0.01] = 2.58;
}
}}}
= Evaluation traps =
An excessive obsession with evaluation can stunt the development of novel ideas.

    * Sadly, this is a rare problem.

= Not enough browsing =

Running scripts and equations is no substitute for looking at the raw distributions:

    * Often, there are surprises in the data that the standard statistical methods never see.
          * e.g. MRE from ten instances = 11,15,18,20,23,25,29,31,100,100
                * then the PRED(30) = 80%
                * and the evaluation misses two disasters (100% MRE- total error).
    * e.g. all the following plots have a correlation between {x,y} of 0.81

= Mis-guided measures of success =

Some evaluation methods are actually quite uninformative

    * e.g. PRED(N)
          * rewards theories that predict close to actual (says nothing about how bad the bad cases are)
    * e.g. MMRE
          * insults theories where predictions are far away from actual (says nothing about how good the good cases are)
    * e.g. Accuracy a very poor measure of theory performance when the target class is very rare.
          * e.g. 10 target classes in 990 instances
          * if we miss all the target, and be silent all the time, A=990, B=10, D=0 , A+B+C+D=1000
          * accuracy = (A+D) / (A+B+C+D) = (990+0)/1000 = 99.9% accuracy
          * e.g. the above roc-sheet where A=395, B=67, C=19, D=39
          * accuracy = 83% but the probability of finding the target is only 37%.
          * e.g. here's one study were hundreds of detectors were built many ways for one data set (including LSR, model trees, Attribute over threshold, etc,etc) and scored by {accuracy,pd,pf,effort} (effort being what % of the code was falgged as "defect-prone").
                * note how accuracy can remain constant while PD, PF change dramatically.

= The World is Not Normal =

For non-normal distributions, use signed ranked tests instead of raw numbers.

Suppose some treatment A generates N1=5 values 
{5,7,2,0,4} and treatment B 
generates N2=6 values 
{4,8,2,3,6,7}, then these sort as 
follows:
{{{
Samples A A B B A B A B A B B 
Values  0 2 2 3 4 4 5 6 7 7 8 
}}}


{{{
Samples A A   B   B A   B   A B A   B   B 
Values  0 2   2   3 4   4   5 6 7   7   8 
Ranks   1 2.5 2.5 4 5.5 5.5 7 8 9.5 9.5 11 
}}}
Then, for _paired_ experiments (same data, different treatments) use Wilcoxon and for _non-paired_
use Mann-Whitney (shown below):

<img width=400 src="http://ourmine.googlecode.com/svn/trunk/share/img/mwu.png">

For full source  see [http://ourmine.googlecode.com/svn/trunk/our/lib/awk/ranked.awk].

