#summary Data Mining : The Missing Link in Empirical Software Engineering

<wiki:toc max_depth="4" />

<img align=middle src="http://www.ksu.edu.sa/sites/Colleges/AppliedMedicalSciences/PublishingImages/under-construction.jpg" width=200>

*NOTE: This 
draft is not yet released.*


= Abstract =

We distinguish "theories" (which are "true" in some global sense) from 
"models" (which may not be "true", be are useful for some local task).
Software engineering research rarely produces externally valid theories.
There are so very few examples of generalizable theories,
despite there being so many examples of research groups trying to find them.
To illustrate this point, we offer examples of these failures-to-generalize from the fields of:
 * design of programming languages
 * software metrics definitions
 * and other SE domains, as well.

Even if SE can't generate theories, we can still find useful local models.  Data mining
is a rich area of research that has generated long lists of methods that are simple to code and, usually, generate
effective models.  This talk will offer:
 * Examples of those local conclusions (as applied to software defect prediction), as well as
 * Catalogs of data mining methods that are simplest and fastest to apply, while often generating useful insights.

Advocating the generation of local models, that usually do not generalize to theories, runs counter to the positivist
tradition of scientific development.  A detailed look of the mechanics of data mining will highlight the difficulty
in producing generalizable theories. 

I am not the first to argue that this tradition is sterile- but I
do not think that the SE culture fully acknowledges the failure of that approach.  For example, we still train students
in classic design of experiments, without acknowledging that such classic designs are usually post-hoc reports
of a far more chaotic process.
Also, our textbooks on empirical methods spell out in tedious details the steps required for empirical studies,
but do not stress the embaressing fact that so few of those studies have produced generalizable theories.

But I my message is not one of dispair. While all models may be wrong, many more of them are not right.
Space of possible models smaller than you might thing.
Rather, it is a claim that we still critically 
it turns out that there are general theories about finding locally useful models.  
To illustrate this point, this talk will:
 * Discusses one such method from case-based reasoning, as applied to software effort estimation.

My conclusion will be that:
 * While we should strive to build general theories of software engineering, 
 * We should not be surprised or discouraged if we fail to do so. 

More generally, rather than try to "clean up" empirical software engineering with more rigorous methods, we should
instead explore methods for the faster generation and assessment of local models.

_About the speaker_ Dr. Tim Menzies (tim@menzies.us) has been working on advanced modeling and AI since 1986. He received his PhD from the University of New South Wales, Sydney, Australia and is the author of over 164 refereed papers.
A former research chair for NASA, Dr. Menzies is now a associate professor at the West Virginia University's Lane Department of Computer Science and Electrical Engineering.
For more information, visit his web page at http://menzies.us.

= Introduction =

Remember of the old joke?

http://ourmine.googlecode.com/svn/trunk/share/img/thenAMiracle.jpg

If you read the empirical SE literature, you can find a similar "missing link" in 
[#Easterbrook07 recent descriptions of how to conduct empirical SE studies]:
  * 3 pages: research questions
  * 2 pages: different forms of "empirical truth"
  * 1 page: role of theory building
  * 9 pages: selecting methods
  * 1 page: data collection techniques
  * 0 pages: data analysis (and then a miracle happens)
  * 2 pages: empirical validity
  * 1 page: conclusions

Speaking as a data mining researcher, I'm here to say that selecting _data analysis_ methods deserves more than 0 pages:

 * There are [http://promisedata.org/2010/ entire conferences] devoted to just "data analysis";
 * A detailed study of "data analysis" reveals:
   * Data analysis may never produce generalizable SE theories 
     * Shock! Horror!
     * (And if such generalizations exist, shouldn't we have found them by now?)
 * Methods of "data analysis" are close to methods of "control"
   * So "empirical methods" becomes "how to control a project".

----
<img align=right src="http://ourmine.googlecode.com/svn/trunk/share/img/death2powerpoint.png">
= Why Aren't we Looking at Powerpoint? =


Like many before me, ([#Tufte05], [#Tufte06]),
I distrust Powerpoint:
 * Powerpoint is a tool for a one-way broadcast of completed ideas 
 * Knowledge diamonds;
 * Dressed to impress.

This format is faster to write.
 * Faster to hyperlink to references
 * You can write comments on this material, below.

Anyway,  this talk comes from a different intellectual tradition:
 * Knowledge relativism
   * Using the hammer changes the hammer.

Seems inappropriate to dress it up in Powerpoint

<br><br>
<br><br>
<br><br>
<br><br>
<br><br>
<br><br>
<br><br>
----
= References =

== Easterbrook07 ==

Easterbrook, S. M., Singer, J., Storey, M, and Damian, D. Selecting Empirical Methods for Software Engineering Research. Appears in F. Shull and J. Singer (eds) "Guide to Advanced Empirical Software Engineering", Springer, 2007.
http://www.cs.toronto.edu/~sme/papers/2007/SelectingEmpiricalMethods.pdf

== Tufte05 ==

Tufte, E. Powerpoint Does Rocket Science--and Better Techniques for Technical Reports.  September 6, 2005.
http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001yB&topic_id=1&topic=Ask+E.T.

== Tufte06 ==
Tufte, E. The Cognitive Style of Powerpoint: Pitching Out Corrupts Within, 2006.
http://www.edwardtufte.com/tufte/books_pp
