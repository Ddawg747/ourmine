#summary stuff to change next time around

 <img  align=middle src="http://chart.apis.google.com/chart?cht=tx&chl=x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}">

== vector space model ==

We have a view that the data more important than the algorithms. Therefore, we view DM as operations on tables of data. Each row of that table is a point in a hyper-dimensional space. Our job is to explore that space to discover its structure.

 * base methods
   * slice, recursively: oner.
   * build one function per class, classify by returning the class whose function returns the highest value: nb, byperpipes
   * float in, look around (NN)

 * rows
   * delete 
      * under-sampling
      * instance sampling
      * anomaly detection
   * weight
      * kernel functions in NN
   * group
      * clustering
   * add 
      * use cluster centroids
      * over-sampling
      * prototype generation
      * active learning
 * columns
     * delete
        * feature selection
     * weight
         * fuzzy selection: b^2/(b+r), infogain, tf*idf
     * group
         * 2R, AODE
     * add
         * classification
            * utility functions (class weighting, n to one)
            * bore: best or rest   
         * rotate:
            * PCA
            * [http://www-connex.lip6.fr/~amini/RelatedWorks/svm_intro.pdf svm kerms]
         * active learning
  
== buzz words ==

 * [http://en.wikipedia.org/wiki/Predictive_analytics Predictive analytics]
 * [http://en.wikipedia.org/wiki/Business_intelligence Business Intelligence]
 * [http://cutting.wordpress.com/2007/07/30/mapreduce-cookbook-for-machine-learning/ Map reduce]

== Background Notes ==

 * http://expdb.cs.kuleuven.be/expdb/index.php
 * Kolmogorov complexity: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.3114

== support vector machines == 

The ideaof SVmachines: map the training data into a higher- 
dimensional feature space viaÎ¦, and construct a separating hyperplane with 
maximum margin there.This yields a nonlinear decision boundary in input space. 
By the use of a kernel function (1.2), it is possible to compute the separating 
hyperplane without explicitly carrying out the map into the feature space. 

Figs 1.4 are good and fig 1.5 and fig 1.2

  * http://www-connex.lip6.fr/~amini/RelatedWorks/svm_intro.pdf

== resources ==

 * http://metaoptimize.com/qa/questions/?sort=mostvoted

== evaluation ==

 * http://www.site.uottawa.ca/~cdrummon/pubs/jetai09.pdf

== Org changes ==

 * more student control of projects
    * generic project. find a user who cares about something. get rules from them (hypotheses) that reflect their current policies. get data from them. maybe do a GAC and generate at random half way between any parent/child (so higher in the tree is more abstract). learn counter examples. build classifier that learns to pick and reject counter examples (use that as a reality filter on the generator). 
 * give them a  text book
 *  weekly homeworks (400)
 * give tutorials on my lisp and ourmine
 * active learning: http://hunch.net/~active_learning/

== classifiers ==

 * winnow: http://citeseerx.ist.psu.edu/viewdoc/similar?doi=10.1.1.21.1166&type=ab

== FSS ===

 * SS for continuous classes http://researchcommons.waikato.ac.nz/handle/10289/1024

== Content changes ==

 * adversarial   search : http://www.cs.washington.edu/homes/pedrod/papers/kdd04.pdf
 * discretization: [YangWebb09]

=== K-means ===

 * single pass k-menas: (also, a good example of a research paper) :http://www.sigmod.org/disc/disc01/out/websites/kdd_explorations_2/farnstrom.pdf
 * triangle inequality and k-means : http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.8422&rep=rep1&type=pdf
 * alternatives to k-meanshhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.2789&rep=rep1&type=pdf
 * other classifiers http://citeseerx.ist.psu.edu/viewdoc/similar?doi=10.1.1.125.9225&type=ab

== model trees ==

Incremental Learning of Linear Model Trees
Machine Learning
Issue	Volume 61, Numbers 1-3 / November, 2005 
Pages	5-48
Duncan Potts1  and Claude Sammut. 05pottsSammut.pdf

== simpler code ==

{{{
(defmacro doitems ((one n list &optional out) &body body )
  `(let ((,n -1))
     (dolist (,one ,list ,out)  (incf ,n) ,@body)))

(defmacro dohash ((key value hash &optional end) &body body)
  `(progn (maphash #'(lambda (,key ,value) ,@body) ,hash)
          ,end))

(defmacro h+ (key hash &optional (n 1))
  `(incf (gethash ,key ,hash ,n)))

;;;;
(defun file->lists (f)
  (with-open-file (str f) 
    (stream->list  str)))

(defun stream->list (str &optional 
                     (line (read-line str nil)))
  (when line
      (cons (string->list line)
            (stream->list str))))

(defun string->list (line)
  (read-from-string
   (concatenate 'string "(" (reverse line) ")")))

;;;;
(defstruct (data (:print-function data-print)) 
  rows 
  (n 0) 
  (classes (make-hash-table)) 
  (h (make-hash-table :test 'equal)))

(defun data-print (d s k)
  (declare (ignore k))
  (format s "#S~a" 
          `(data n ,(data-n d) classes ,(data-classes d))))

(defun file->data (file &optional (label #'identity))
  (lists->data 
   (make-data :rows (file->lists file)) 
   label))

(defun lists->data (dat label)
  (dolist (list (data-rows dat) dat)
    (when list
      (list->data list dat label))))

(defun list->data (list dat label)
  (let ((class (funcall label (first list))))
    (incf (data-n dat))
    (h+ class (data-classes dat))
    (doitems (one n (rest list))
        (h+ `(,class ,n ,one) (data-h dat)))))

(defun vowelp (x) (member x '(a e i o u)))

;;;;
(defun test1 ()
  (file->data  
   "letter.dat"
   #'(lambda (x)
       (if (vowelp x) 1 0))))

(test1)
}}}