\subsection{Introduction}
It may seem unfair to consider an algorithm that in many ways, cheats. While most classifiers are burdened with the task of predicting for a single class, it is possible for our algorithm to return all possible classes and claim victory in a meaningless fashion. While we will demonstrate that this extreme overclassifying is rarely the case, it stands to reason that comparisions cannot be made between the accuracy of our disjunctions and those of single classifiers.

However, just as HyperPipes had it's niche in sparse data, MultiPipes has a niche in reducing the space of possible classifications. Consider the task of matching a photo of a criminal to a database of millions. While returning a single match with prosecution strength accuracy would be ideal, reducing the matches to a handful of twenty or thirty would be nearly as useful. At that point the problem becomes tractable for a human, and our propensity for pattern matching can now supplement the learner. In other words, our best interest may not always lie in trusting the smartest learner in the room; the hubris of certainty ignores our own innate abilities of classification.

\subsection{Disjunctive Naive-Bayes}
Due to the success of turning HyperPipes into MultiPipes we thought we might be able to apply some of the same methods to Naive Bayes, specifically our Alpha measure described above. The idea being that we return any classes within a certain percentage of the highest scoring class. This method proved to be minimaly effective as the spread between the highest rated class and the next highest rated class tended to be very large meaning even a high alpha rarely caused the returned set to be larger than a single classification.
\subsection{Measuring Performance}
