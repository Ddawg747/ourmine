\subsection{Introduction}
It may seem unfair to consider an algorithm that in many ways, cheats. While most classifiers are burdened with the task of predicting for a single class, it is possible for our algorithm to return all possible classes and claim victory in a meaningless fashion. While we will demonstrate that this extreme overclassifying is rarely the case, it stands to reason that comparisions cannot be made between the accuracy of our disjunctions and those of single classifiers.

However, just as HyperPipes had it's niche in sparse data, MultiPipes has a niche in reducing the space of possible classifications. Consider the task of matching a photo of a criminal to a database of millions. While returning a single match with prosecution strength accuracy would be ideal, reducing the matches to a handful of twenty or thirty would be nearly as useful. At that point the problem becomes tractable for a human, and our propensity for pattern matching can now supplement the learner. In other words, our best interest may not always lie in trusting the smartest learner in the room; the hubris of certainty ignores our own innate abilities of classification.

\subsection{Disjunctive Naive-Bayes}
\subsection{Measuring Performance}
