HyperPipes is a learner originally designed by Witten\cite{Witten99} and implemented by Eisenstein et al~\cite{Eisenstein04} for extremely large, sparse datasets. Rather than maintain a large working memory of statistics on each row of data, HyperPipes maintains a small data structure for each class that merely "remembers" whether a particular attribute has been encountered before. For numerics, a range of maximum and minimum values encountered is kept. When classifying, original HyperPipes classifies a row based on which class most "contains" the current attributes. For numeric attributes, a new instance is "contained" if it falls within the maximum and minimum values see so far.

Hyperpipes works well for sparse datsets, as your working memory need only contain one HyperPipe for each possible class, and each pipe must maintain only a the unique symbols encountered so far along with two numeric bounds for each column. The result is a fast, dumb, scalable learner.

One caveat remains in the HyperPipes algorithm. For large, sparse datasets there are enough unique columns to promote a wide variance in which HyperPipe best "contains" a class. However, for more traditional datasets with fewer columns HyperPipes' accuracy breaks down. By nature HyperPipes is strongly succeptible to outlier data, as the frequency of an encountered attribute is ignored and extremely high or low numerics will stretch the bounds. In this paper we explore a means to maintain the speed and scalability of HyperPipes while extending it's application to a wider variety of datasets.
